{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import io                       #for input output\n",
    "import re                       #for regular expression matching\n",
    "from collections import Counter #for counting elements\n",
    "import nltk                     #for tokenization\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')          #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract string from the corpos file\n",
    "#and return as a string\n",
    "def read_corpus_file(file_path):\n",
    "    text = \"\"\n",
    "    with io.open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract all the words from the corpus file \n",
    "and return as a list \n",
    "'''\n",
    "\n",
    "def get_corpus_words(plain_text):\n",
    "    corpus_words = []\n",
    "    \n",
    "    \n",
    "    sentences = plain_text.split('.')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenizing method 1\n",
    "        # Tokenize words within each sentence using split()\n",
    "        words = sentence.split()\n",
    "        \n",
    "        # Separate words where there is a forward slash (\"/\") between them\n",
    "        separated_words = []\n",
    "        for word in words:\n",
    "            if '/' in word:\n",
    "                separated_words.extend(re.split(r'/', word))\n",
    "            else:\n",
    "                separated_words.append(word)\n",
    "        \n",
    "        # Tokenizing method 2\n",
    "        # Tokenize words within each sentence using word_tokenize\n",
    "        tokenized_words = word_tokenize(' '.join(separated_words).lower())\n",
    "        \n",
    "        # Remove non-alphabetical characters except hyphen and apostrophe\n",
    "        cleaned_words = [re.sub('[^a-zA-Z\\-\\'\\s]', '', word) for word in tokenized_words]\n",
    "        \n",
    "        # Filter out empty words\n",
    "        filtered_words = [word for word in cleaned_words if word]\n",
    "        \n",
    "        # Append words to corpus_words\n",
    "        corpus_words.extend(filtered_words)\n",
    "    \n",
    "    return corpus_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the corpus file as string\n",
    "file_path = \"corpus 3.txt\"\n",
    "plain_text = read_corpus_file(file_path)\n",
    "\n",
    "print(len(plain_text))\n",
    "plain_text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract all the word from the plain text corpus file \n",
    "and return as a list \n",
    "'''\n",
    "\n",
    "list_of_words = get_corpus_words(plain_text)\n",
    "print(len(list_of_words))\n",
    "print(list_of_words[0:20])\n",
    "print(list_of_words[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurrence of each word\n",
    "#building the dictionary of words\n",
    "\n",
    "WORDS_collections = Counter(list_of_words)\n",
    "WORDS_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define Spell Checker class\n",
    "Doing all the calculation like probability, words_in_dictionary words, \n",
    "edits, correction and generating candidates\n",
    "'''\n",
    "\n",
    "class SpellChecker:\n",
    "    \n",
    "    def __init__(self, term_freq):\n",
    "        self.w_rank = {}\n",
    "        self.letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        \n",
    "        N = sum(term_freq.values())\n",
    "        for term in term_freq:\n",
    "            self.w_rank[term] = term_freq[term] / N\n",
    "    \n",
    "    #probability of occurance of a given word based on its frequency\n",
    "    def probability(self, word): \n",
    "        return self.w_rank.get(word, 0)\n",
    "\n",
    "\n",
    "    '''\n",
    "    takes a list of word and return a set of words that appear\n",
    "    in the WORDS_collections dictionary\n",
    "    '''\n",
    "    def words_in_dictionary(self, words): \n",
    "        \"The subset of 'words' that appear in the dictionary of w_rank.\"\n",
    "        return set(w for w in words if w in self.w_rank)\n",
    "\n",
    "\n",
    "    '''\n",
    "    the follwoing function generates all possible correction_set that\n",
    "    are one correction away from given the wrong word\n",
    "    then it create all the correction \n",
    "    ''' \n",
    "    def correction_1(self, word):\n",
    "        \"All edits that are one edit away from 'word'.\"\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "\n",
    "        # peforming deletion\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "\n",
    "        #performing transposition\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "\n",
    "        #performing Replacement\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in self.letters]\n",
    "\n",
    "        #performing Insertion\n",
    "        inserts    = [L + c + R               for L, R in splits for c in self.letters]\n",
    "        \n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "    '''\n",
    "    the follwoing function generates all possible edits that\n",
    "    are two correction away from given the wrong word\n",
    "    then it create all the correction using correction_1()\n",
    "    '''\n",
    "    def correction_2(self, word): \n",
    "        \"All edits that are two edits away from 'word'.\"\n",
    "        return (e2 for e1 in self.correction_1(word) for e2 in self.correction_1(e1))\n",
    "    \n",
    "\n",
    "    '''\n",
    "    select the correction word with the highest probability\n",
    "    using probability()\n",
    "    '''\n",
    "    def correction(self, word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key = self.probability)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    generate list of possible correction for given word\n",
    "    it use words_in_dictionary function to weather the word is already in the\n",
    "    dictionary otherwise it use correction_1, correction_2\n",
    "    '''\n",
    "    def candidates(self, word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.words_in_dictionary([word]) or self.words_in_dictionary(self.correction_1(word)) or self.words_in_dictionary(self.correction_2(word)) or [word])\n",
    "    \n",
    "\n",
    "    #correct misspelled word in a sentence\n",
    "    def correct_sentence(self, sentence):\n",
    "        \"Correct misspelled words in a sentence.\"\n",
    "        corrected_sentence = []\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            corrected_word = self.correction(word.lower())\n",
    "            corrected_sentence.append(corrected_word)\n",
    "        return ' '.join(corrected_sentence)\n",
    "    \n",
    "    #counting how many word is corrected\n",
    "    def count_changed_words(self, corrected_words, error_words):\n",
    "        changed_count = 0\n",
    "        for corrected_word in corrected_words:\n",
    "            if corrected_word not in error_words:\n",
    "                changed_count += 1\n",
    "        return changed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "sp_model = SpellChecker(WORDS_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability of the word 'the'\n",
    "sp_model.probability('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_collections['denied']\n",
    "WORDS_collections['Thiss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability of the word 'unmentioned'\n",
    "sp_model.probability('denied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that exist in the dictionary\n",
    "sp_model.words_in_dictionary(['the', 'unmentioned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp_model.correction('siter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get candidates for word 'wlak'\n",
    "sp_model.candidates('wlak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model.correct_sentence('Thiss is a samplee textt with some incorect speling and gramatical mistkes. I am testiing the spelling corction function. Hopfully, it will corect the errrors and imrove the accuracy of the texxt.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before going to the accuracy part, there is some clarification to be made. Because to calculate the accuracy, a ground truth is required(must). Without a ground truth i can not compare  the corrected word. Also i can not compare the corrected with the corpus file as the corrected words are taken from the corpus file. If I do so the accuracy will be 100%. So to calculate the accuracy i took a misspelled sentence and a corrected sentence which is the fully corrected version of the misspelled sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So to calculate the accuracy i used another english dictinary as ground truth. i will provide the link of that. So what i did is that i checked the corrected word if that one exists in the ground truth. if the corrected word is in the ground truth then the word is corrected successfully otherwise not.\n",
    "\n",
    "english dictionary link which i used as ground truth:  https://www-personal.umich.edu/~jlawler/wordlist\n",
    "and i rename the file as ground_truth_from_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the ground truth file\n",
    "ground_truth_file_path = \"ground_truth_from_online.txt\"\n",
    "ground_truth_text = read_corpus_file(ground_truth_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_words = ground_truth_words = sorted(set(word_tokenize(ground_truth_text.lower())))\n",
    "print(len(ground_truth_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using binary search to search a word in the ground truth file\n",
    "def binary_search(word, ground_truth_words):\n",
    "    low = 0\n",
    "    high = len(ground_truth_words) - 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_word = ground_truth_words[mid]\n",
    "\n",
    "        if mid_word == word:\n",
    "            return True\n",
    "        elif mid_word < word:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking bu=inary search\n",
    "binary_search('hello', ground_truth_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the error file and correct it\n",
    "error_file = \"error_file.txt\"\n",
    "error_file = read_corpus_file(error_file)\n",
    "\n",
    "# Get the unique words from the plain text\n",
    "error_words = get_corpus_words(error_file)\n",
    "list_of_words.sort()\n",
    "\n",
    "\n",
    "total_error_count = 0\n",
    "error_words_string = \"\"\n",
    "\n",
    "for word in error_words:\n",
    "    if not binary_search(word, list_of_words):\n",
    "        total_error_count += 1\n",
    "        error_words_string += word + \" \"\n",
    "\n",
    "print(\"Number of error words:\", total_error_count)\n",
    "# print(\"Error words:\", error_words_string)\n",
    "\n",
    "corrected_text = sp_model.correct_sentence(error_file)\n",
    "error_string_c = sp_model.correct_sentence(error_words_string)\n",
    "\n",
    "#save the corrected file\n",
    "corrected_file_path = \"corrected_file.txt\"\n",
    "with open(corrected_file_path, \"w\") as corrected_file:\n",
    "    corrected_file.write(corrected_text)\n",
    "\n",
    "error_string_c_tokenize = word_tokenize(error_string_c)\n",
    "# print(error_string_c_tokenize)\n",
    "\n",
    "#calculating the accuracy\n",
    "correct_count = 0\n",
    "for word in error_string_c_tokenize:\n",
    "    # print(word)\n",
    "    if binary_search(word, ground_truth_words):\n",
    "        correct_count += 1\n",
    "# print(correct_count)\n",
    "accuracy = (correct_count/total_error_count) * 100\n",
    "\n",
    "print(\"Error text: \\n\" + str(error_file))\n",
    "print()\n",
    "print(\"Corrected text: \\n\" + str(corrected_text))\n",
    "print()\n",
    "print('Accuracy = ' + str(accuracy) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
